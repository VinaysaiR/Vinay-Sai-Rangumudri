{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75087112",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas \n",
    "#Panel data --> this is a library that is mainly used for data manipulation\n",
    "#and also it has some features for data visualization and it is mainly used\n",
    "#in EDA\n",
    "\n",
    "#2--> Series ,#Dataframe \n",
    "#Series is ____ \n",
    "#1.Unidimensional \n",
    "#2.Multidimensional\n",
    "\n",
    "#Dataframe is bidimensional \n",
    "\n",
    "#Panels is 3 dimensional \n",
    "\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5193fc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A USECASE FOR PANDAS??\n",
    "#CRISP DM --> Cross Industry Standard Process for Data Mining \n",
    "\n",
    "#Data Lifecycle \n",
    "#Problem statement \n",
    "#Analyse and predict the sales for future based on last years data\n",
    "\n",
    "#Sales --> Analysed and then built a ml model on it based on last years data\n",
    "\n",
    "\n",
    "#DB ---> TONS OF DATA(Tons means lots and lots of data)\n",
    "\n",
    "#Data Modeling--> Getting a rough idea about what data is required\n",
    "\n",
    "#Sales-->\n",
    "#Date , #Amount of Sales #Product #Profit #Region \n",
    "\n",
    "#DB--> SQL TO FETCH THE DATA I NEED \n",
    "\n",
    "#Collecting data --> DB, Scrape out the data \n",
    "\n",
    "#SQL,Python--> Data Cleaning \n",
    "\n",
    "#Python --> \n",
    "#Connect the data\n",
    "#pd.read_csv \n",
    "#pd.read_xlsx\n",
    "\n",
    "#dataframe --> \n",
    "\n",
    "#Inspecting the data\n",
    "\n",
    "#df.head()--> displaying top records \n",
    "\n",
    "#df.tail()--> displaying bottom records \n",
    "\n",
    "#df.info()---> entire insights about the data types of df as well as no of not null values \n",
    "#memory occupied \n",
    "\n",
    "#df.shape()--> return the shape of the data frame \n",
    "\n",
    "#df.isnull().sum()--> return the no of null values col wise for the data frame \n",
    "\n",
    "#df.describe()---> return statistical summary of the data (numerical col all except for obj) ) \n",
    "#df.info()--> 3 dtypes--> int64, float64, object \n",
    "#All those refering to object --> ordinal col #text or string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204daca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efed268",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#null values -->\n",
    "#data cleaning --> 60% time in cleaning the data Why???\n",
    "#Data cleaning --> no null values as well no outliers so due to these reason it will be quiet efficient for us to generate some insights\n",
    "\n",
    "#Data cleaning --> 3 steps--> dealing with null values \n",
    "#3--> drop col--> df.drop(series_name,axis=1) #axis=1==col #null values more than 50% of the actual data\n",
    "#drop na()--> drop the values --> 2% to 5% #1000 --> 20 null - 50 null --> df.dropna()\n",
    "#5% to 50%--> df.fillna() #4 --> ffill --> it will fill the value with succedding value (np.nan, 5) (5,5)\n",
    "#bfill --> it will fill the values with the precedding values --> (4,np.nan)--> (4,4)\n",
    "#mean---> if there is no outliers --> use mean \n",
    "#median --> in case of outlier --> use the median\n",
    "#ordinal data--> df.fillna(.mode )--> use mode \n",
    "\n",
    "#Dealing up the outliers--> \n",
    "#detecting the outliers--> float 64 ,int 64 ,\n",
    "#outliers --> data is continous --> outliers \n",
    "\n",
    "#detecting the outliers--> df.plot(kind='box ')\n",
    "#df.plot(kind='hist')\n",
    "\n",
    "#you will get a tail--> skewness --> there is an outlier\n",
    "\n",
    "#drop the row val\n",
    "#fill it with median \n",
    "\n",
    "#change the col--> \n",
    "#1 2 3 4 5 \n",
    "#give the col name\n",
    "\n",
    "#df.columns=[list of the names of columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c452f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 data frames --> \n",
    "#combining the two dataframes--->\n",
    "#pd.merge(df_name,df_name2,on=common_col,how='inner ','outer--> union ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd8c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "#we will just convert one value to another \n",
    "#replace certain values \n",
    "\n",
    "#df.replace(actual_val,new_val)\n",
    "#we have a few col--> data type --> obj\n",
    "#zero, one,two,three,four\n",
    "#replace these values with the numbers---> object dtype gets converted into numerical dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caef633",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA --> Exploratory data analysis --> converting the data into insights \n",
    "#Univariant analysis--> uni(1)-- variant(col)\n",
    "#analysis of one col\n",
    "\n",
    "#df['colname'].value_counts() #this will give you the total no of values corresponding to one unique value\n",
    "#(567) #discrete/object \n",
    "#male-->300  \n",
    "#female--> 267\n",
    "\n",
    "#continous --> df.plot(kind='hist') #generating the histogram for each \n",
    "\n",
    "\n",
    "#Bivariant analysis---> #analysis taking 2 cols at a time \n",
    "\n",
    "#bar graph\n",
    "#count plot\n",
    "#line plot \n",
    "\n",
    "#df.plot(x=col_name,y=col_name,kind='line or bar')\n",
    "\n",
    "#df.groupby (col1,col2) #group by function will take into consideration the col \n",
    "#and it will create groups corresponding to a val\n",
    "\n",
    "#multivariant analysis--> 2 or more columns\n",
    "\n",
    "#df.corr() #it will give the correlation between all the columns \n",
    "\n",
    "#sns.pairplot(df)\n",
    "\n",
    "#df.groupby(col1)\n",
    "\n",
    "#3 assesment\n",
    "\n",
    "#1 CSV FILE\n",
    "\n",
    "#you have to generate some meaningful insights out of it \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bffc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thus we conclude that,\n",
    "#list of points that you got by eda on the data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
